{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWE3oy+J2v3Zm52J77m6Tm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MyDarapy/Cereals-EDA-and-Data-Modelling-/blob/main/FFN_for_MNIST_Digits_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Implementing a multilayer neural network (Feedforward Neural Nets) to do a digit classification on the MNIST dataset.\n",
        "This FFNN implements stochastic gradient descent learning algorithm using just numpy. Error back propagation is used for computing the gradient'''"
      ],
      "metadata": {
        "id": "QvZY_L1X11C9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "cc9d6608-0907-4bd0-f013-99db5850da5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Implementing a multilayer neural network (Feedforward Neural Nets) to do a digit classification on the MNIST dataset.\\nThis FFNN implements stochastic gradient descent learning algorithm. Error back propagation is used for computing the gradient'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "VTmlXTpM_QjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(object):\n",
        "  def __init__(self, sizes):\n",
        "    self.num_layers = len(sizes)\n",
        "    self.sizes = sizes\n",
        "    self.biases = [np.random.randn(y, 1) for y in sizes [1:]]\n",
        "    self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "\n",
        "  def feedforward(self, a):\n",
        "    for b, w in zip(self.biases, self.weights):\n",
        "      a = sigmoid(np.dot(w, a)+b)\n",
        "    return a\n",
        "\n",
        "  def SGD(self, training_data, epochs, mini_batch_size, eta, test_data= None):\n",
        "    training_data = list(training_data)\n",
        "    n = len(training_data)\n",
        "\n",
        "    if test_data:\n",
        "      test_data = list(test_data)\n",
        "      n_test= len(test_data)\n",
        "\n",
        "      random.shuffle(training_data)\n",
        "      mini_batches = [training_data[k:k+mini_batch_size] for k in range (0, n, mini_batch_size)]\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      for mini_batch in mini_batches:\n",
        "        self.update_mini_batch(mini_batch, eta)\n",
        "      print(\"Loss after {} epoch: {}\".format(epoch, self.loss()))\n",
        "      if test_data:\n",
        "        print(\"Epoch {}: {}/{}\".format(epoch, self.evaluate(test_data), n_test))\n",
        "      else:\n",
        "        print(\"Epoch {} complete\".format(epoch))\n",
        "\n",
        "\n",
        "  #The update_mini_batch code that performs weight updates on the data in each batch\n",
        "  def update_mini_batch(self, mini_batch, eta):\n",
        "\n",
        "    grad_b = [np.zeros(b.shape) for b in self.biases]\n",
        "    grad_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "    for x, y in mini_batch:\n",
        "      delta_grad_b, delta_grad_w = self.backprop(x, y) #Change in the weights and biases with to the cost function.\n",
        "      grad_b = [gb+dnb for gb, dnb in zip(grad_b, delta_grad_b)] #this is doing the accumulation\n",
        "      grad_w = [gw+dnw for gw, dnw in zip(grad_w, delta_grad_w)]\n",
        "\n",
        "    self.weights = [w-(eta/len(mini_batch))*gw for w, gw in zip(self.weights, grad_w)]\n",
        "    self.biases= [b-(eta/len(mini_batch))*gb for b, gb in zip(self.biases, grad_b)]\n",
        "\n",
        "  def backprop(self, x, y):\n",
        "    grad_b = [np.zeros(b.shape) for b in self.biases]\n",
        "    grad_w = [np.zeros(w.shape) for w in self.weights]\n",
        "    # feedforward\n",
        "    activation = x\n",
        "    activations = [x] # list to store all the activations, layer by layer\n",
        "    zs = [] # list to store all the z vectors, layer by layer\n",
        "    for b, w in zip(self.biases, self.weights):\n",
        "      z = np.dot(w, activation)+b\n",
        "      zs.append(z)\n",
        "      activation = sigmoid(z)\n",
        "      activations.append(activation)\n",
        "\n",
        "        # backward pass\n",
        "    delta = self.cost_derivative(activations[-1], y) * \\\n",
        "    sigmoid_prime(zs[-1])\n",
        "    grad_b[-1] = delta\n",
        "    grad_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "\n",
        "    for l in range(2, self.num_layers):\n",
        "      z = zs[-l]\n",
        "      sp = sigmoid_prime(z)\n",
        "      delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
        "      grad_b[-l] = delta\n",
        "      grad_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "    return (grad_b, grad_w)\n",
        "\n",
        "  def evaluate(self, test_data):\n",
        "    test_results = [(np.argmax(self.feedforward(x)), y)\n",
        "                        for (x, y) in test_data]\n",
        "    return sum(int(x == y) for (x, y) in test_results)\n",
        "\n",
        "  def cost_derivative(self, output_activations, y):\n",
        "    return (output_activations-y)\n",
        "\n",
        "  #def loss(self, activations, y):\n",
        "    #calculate_loss = (activations[-1] - y)\n",
        "    #return calculate_loss\n"
      ],
      "metadata": {
        "id": "3ZGAtWxd_bel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "  return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "  return sigmoid(z)*(1-sigmoid(z))"
      ],
      "metadata": {
        "id": "bsRONPx-df_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mnist_loader"
      ],
      "metadata": {
        "id": "EwiQG4Ts4SyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()"
      ],
      "metadata": {
        "id": "SCliXeMl4N3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = NeuralNetwork([784, 30, 10]) #[Input size, hidden_size, output_size]"
      ],
      "metadata": {
        "id": "4LJEOvG88MTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net.SGD(training_data, 30, 10, 3.0, test_data = test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TxSj_Is_EEQ",
        "outputId": "3565a84e-a95d-4614-c39e-67cffc6845a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: 9021/10000\n",
            "Epoch 1: 9197/10000\n",
            "Epoch 2: 9298/10000\n",
            "Epoch 3: 9312/10000\n",
            "Epoch 4: 9324/10000\n",
            "Epoch 5: 9342/10000\n",
            "Epoch 6: 9373/10000\n",
            "Epoch 7: 9356/10000\n",
            "Epoch 8: 9383/10000\n",
            "Epoch 9: 9363/10000\n",
            "Epoch 10: 9382/10000\n",
            "Epoch 11: 9405/10000\n",
            "Epoch 12: 9422/10000\n",
            "Epoch 13: 9419/10000\n",
            "Epoch 14: 9411/10000\n",
            "Epoch 15: 9416/10000\n",
            "Epoch 16: 9409/10000\n",
            "Epoch 17: 9394/10000\n",
            "Epoch 18: 9403/10000\n",
            "Epoch 19: 9413/10000\n",
            "Epoch 20: 9432/10000\n",
            "Epoch 21: 9430/10000\n",
            "Epoch 22: 9456/10000\n",
            "Epoch 23: 9429/10000\n",
            "Epoch 24: 9432/10000\n",
            "Epoch 25: 9456/10000\n",
            "Epoch 26: 9452/10000\n",
            "Epoch 27: 9450/10000\n",
            "Epoch 28: 9437/10000\n",
            "Epoch 29: 9413/10000\n"
          ]
        }
      ]
    }
  ]
}